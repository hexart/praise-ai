# Praise AI API å‚è€ƒæ–‡æ¡£

## ğŸ“‹ ç›®å½•

- [å‰ç«¯API](#å‰ç«¯api)
- [åç«¯API](#åç«¯api)
- [æ•°æ®ç±»å‹å®šä¹‰](#æ•°æ®ç±»å‹å®šä¹‰)
- [é”™è¯¯ä»£ç ](#é”™è¯¯ä»£ç )
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)

## ğŸ¯ å‰ç«¯API

### Provideræ¥å£ (IProvider)

#### testConnection()
æµ‹è¯•AIæœåŠ¡è¿æ¥çŠ¶æ€

```typescript
testConnection(): Promise<APIResponse<ConnectionTestResponse>>

// è¿”å›ç±»å‹
interface ConnectionTestResponse {
  status: 'connected' | 'disconnected';
  message?: string;
  latency?: number;
}

// ä½¿ç”¨ç¤ºä¾‹
const result = await provider.testConnection();
if (result.success) {
  console.log('è¿æ¥çŠ¶æ€:', result.data.status);
}
```

#### listModels()
è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨

```typescript
listModels(): Promise<APIResponse<ModelsResponse>>

// è¿”å›ç±»å‹
interface ModelsResponse {
  models: ModelInfo[];
}

interface ModelInfo {
  id: string;
  name: string;
  size?: string;
  description?: string;
  parameters?: string;
}

// ä½¿ç”¨ç¤ºä¾‹
const result = await provider.listModels();
if (result.success) {
  console.log('å¯ç”¨æ¨¡å‹:', result.data.models);
}
```

#### switchModel()
åˆ‡æ¢å½“å‰ä½¿ç”¨çš„æ¨¡å‹

```typescript
switchModel(modelName: string): Promise<APIResponse<void>>

// ä½¿ç”¨ç¤ºä¾‹
const result = await provider.switchModel('llama2');
if (result.success) {
  console.log('æ¨¡å‹åˆ‡æ¢æˆåŠŸ');
}
```

#### sendMessage()
å‘é€æ™®é€šæ¶ˆæ¯ï¼ˆéæµå¼ï¼‰

```typescript
sendMessage(request: ChatRequest): Promise<APIResponse<ChatResponse>>

// è¯·æ±‚ç±»å‹
interface ChatRequest {
  message: string;
  mode: ChatMode;
  userId: string;
  chatHistory: ChatMessage[];
  systemPrompt?: string;
}

// å“åº”ç±»å‹
interface ChatResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

// ä½¿ç”¨ç¤ºä¾‹
const response = await provider.sendMessage({
  message: "ä½ å¥½",
  mode: "smart",
  userId: "user123",
  chatHistory: []
});
```

#### sendStreamMessage()
å‘é€æµå¼æ¶ˆæ¯

```typescript
sendStreamMessage(
  request: ChatRequest,
  onChunk: StreamCallback,
  onMetadata?: MetadataCallback
): Promise<void>

// å›è°ƒç±»å‹
type StreamCallback = (chunk: string, isComplete: boolean) => void;
type MetadataCallback = (metadata: StreamMetadata) => void;

// ä½¿ç”¨ç¤ºä¾‹
await provider.sendStreamMessage(
  request,
  (chunk, isComplete) => {
    console.log('æ¥æ”¶åˆ°å†…å®¹:', chunk);
    if (isComplete) console.log('æµå¼ä¼ è¾“å®Œæˆ');
  }
);
```

### Hooks API

#### useApp()
ä¸»åº”ç”¨çŠ¶æ€ç®¡ç†Hook

```typescript
interface UseAppReturn {
  provider: ReturnType<typeof useProvider>;
  chat: ReturnType<typeof useChat>;
  emotion: ReturnType<typeof useEmotionAnalysis>;
  settings: AppSettings;
  updateSettings: (newSettings: Partial<AppSettings>) => void;
  userId: string;
  isReady: boolean;
  error: string | null;
  resetAll: () => void;
  exportData: () => string;
  importData: (data: string) => boolean;
}

// ä½¿ç”¨ç¤ºä¾‹
const {
  provider,
  chat,
  emotion,
  settings,
  updateSettings
} = useApp();
```

#### useProvider()
Providerç®¡ç†Hook

```typescript
interface UseProviderReturn {
  provider: BaseProvider | null;
  providerType: ProviderType;
  config: ProviderConfig;
  models: ModelInfo[];
  currentModel: string | null;
  isConnected: boolean;
  isLoading: boolean;
  error: string | null;
  supportedProviders: ProviderInfo[];
  switchProvider: (type: ProviderType, config: ProviderConfig) => Promise<boolean>;
  testConnection: () => Promise<boolean>;
  refreshModels: () => Promise<void>;
  switchModel: (modelName: string) => Promise<boolean>;
}

// ä½¿ç”¨ç¤ºä¾‹
const {
  provider,
  models,
  switchProvider,
  testConnection
} = useProvider();
```

#### useChat()
èŠå¤©åŠŸèƒ½Hook

```typescript
interface UseChatReturn {
  chatHistory: ChatMessage[];
  isLoading: boolean;
  error: string | null;
  sendMessage: (message: string, mode: ChatMode) => Promise<void>;
  clearHistory: () => void;
  streamingMessageId: string | null;
  lastDebugInfo: string | null;
}

// ä½¿ç”¨ç¤ºä¾‹
const {
  chatHistory,
  isLoading,
  sendMessage,
  clearHistory
} = useChat({
  provider: provider.provider,
  userId: 'user123'
});
```

#### useEmotionAnalysis()
æƒ…æ„Ÿåˆ†æHook

```typescript
interface UseEmotionAnalysisReturn {
  analysisHistory: EmotionAnalysis[];
  currentEmotion: EmotionAnalysis | null;
  emotionTrend: EmotionTrend | null;
  isAnalyzing: boolean;
  error: string | null;
  analyzeMessage: (message: string) => Promise<EmotionAnalysis>;
  clearHistory: () => void;
  getEmotionService: () => EmotionAnalysisService;
}

// ä½¿ç”¨ç¤ºä¾‹
const {
  analysisHistory,
  currentEmotion,
  analyzeMessage
} = useEmotionAnalysis({
  userId: 'user123',
  autoAnalyze: true
});
```

## ğŸŒ åç«¯API

### åŸºç¡€ä¿¡æ¯

- **åŸºç¡€URL**: `http://localhost:8000`
- **APIç‰ˆæœ¬**: v1
- **è®¤è¯**: æ— éœ€è®¤è¯ï¼ˆæœ¬åœ°ä»£ç†æœåŠ¡ï¼‰
- **Content-Type**: `application/json`

### ç«¯ç‚¹è¯´æ˜

#### å¥åº·æ£€æŸ¥

```http
GET /health
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "status": "healthy",
  "ollama": "connected",
  "api": "openai-compatible",
  "version": "2.0.0"
}
```

#### è·å–æ¨¡å‹åˆ—è¡¨

```http
GET /v1/models
GET /models  # å…¼å®¹è·¯å¾„
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "object": "list",
  "data": [
    {
      "id": "llama2",
      "object": "model",
      "created": 1234567890,
      "owned_by": "ollama"
    },
    {
      "id": "mistral",
      "object": "model", 
      "created": 1234567890,
      "owned_by": "ollama"
    }
  ]
}
```

#### èŠå¤©å®Œæˆï¼ˆéæµå¼ï¼‰

```http
POST /v1/chat/completions
POST /chat/completions  # å…¼å®¹è·¯å¾„
```

**è¯·æ±‚ä½“**:
```json
{
  "model": "llama2",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user", 
      "content": "Hello, how are you?"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 1000,
  "top_p": 1.0,
  "frequency_penalty": 0.0,
  "presence_penalty": 0.0,
  "stream": false
}
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "llama2",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "I'm doing well, thank you! How can I help you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 15,
    "total_tokens": 40
  }
}
```

#### èŠå¤©å®Œæˆï¼ˆæµå¼ï¼‰

```http
POST /v1/chat/completions
Content-Type: application/json
Accept: text/event-stream
```

**è¯·æ±‚ä½“**:
```json
{
  "model": "llama2",
  "messages": [
    {"role": "user", "content": "Write a short story"}
  ],
  "stream": true,
  "temperature": 0.8,
  "max_tokens": 500
}
```

**å“åº”ç¤ºä¾‹** (Server-Sent Events):
```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"llama2","choices":[{"index":0,"delta":{"content":"Once"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"llama2","choices":[{"index":0,"delta":{"content":" upon"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"llama2","choices":[{"index":0,"delta":{"content":" a"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"llama2","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

#### æ–‡æœ¬å®Œæˆ

```http
POST /v1/completions
POST /completions  # å…¼å®¹è·¯å¾„
```

**è¯·æ±‚ä½“**:
```json
{
  "model": "llama2",
  "prompt": "The weather today is",
  "temperature": 0.7,
  "max_tokens": 50,
  "stream": false
}
```

## ğŸ“Š æ•°æ®ç±»å‹å®šä¹‰

### æ ¸å¿ƒç±»å‹

#### ChatMessage
```typescript
interface ChatMessage {
  id: string;
  role: 'user' | 'assistant' | 'system';
  content: string;
  timestamp: number;
  emotion?: EmotionAnalysis;
}
```

#### EmotionAnalysis
```typescript
interface EmotionAnalysis {
  primary_emotion: string;
  intensity: number;         // 0-1
  needs: string;
  confidence: number;        // 0-1
  keywords: string[];
  analysis_source?: string;
  reasoning?: string;
}
```

#### ProviderConfig
```typescript
interface ProviderConfig {
  apiUrl: string;
  apiKey?: string;
  defaultModel?: string;
  timeout?: number;
  headers?: Record<string, string>;
}
```

#### APIResponse
```typescript
interface APIResponse<T> {
  success: boolean;
  data?: T;
  error?: string;
  code?: string;
}
```

### æšä¸¾ç±»å‹

#### ChatMode
```typescript
type ChatMode = 'praise' | 'comfort' | 'smart';
```

#### ProviderType
```typescript
type ProviderType = 'ollama' | 'openai';
```

#### EmotionType
```typescript
type EmotionType = 'happy' | 'sad' | 'angry' | 'anxious' | 'calm' | 'confused' | 'other';
```

## âŒ é”™è¯¯ä»£ç 

### å‰ç«¯é”™è¯¯ä»£ç 

| ä»£ç  | æè¿° | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| `PROVIDER_NOT_AVAILABLE` | Providerä¸å¯ç”¨ | æ£€æŸ¥Provideré…ç½®å’Œè¿æ¥ |
| `MODEL_NOT_SELECTED` | æœªé€‰æ‹©æ¨¡å‹ | é€‰æ‹©ä¸€ä¸ªå¯ç”¨æ¨¡å‹ |
| `NETWORK_ERROR` | ç½‘ç»œè¿æ¥é”™è¯¯ | æ£€æŸ¥ç½‘ç»œè¿æ¥ |
| `SEND_ERROR` | æ¶ˆæ¯å‘é€å¤±è´¥ | é‡è¯•æˆ–æ£€æŸ¥ProviderçŠ¶æ€ |
| `ANALYSIS_ERROR` | æƒ…æ„Ÿåˆ†æå¤±è´¥ | ä¼šè‡ªåŠ¨fallbackåˆ°å…³é”®è¯åˆ†æ |

### åç«¯é”™è¯¯ä»£ç 

| HTTPçŠ¶æ€ç  | æè¿° | åŸå›  |
|------------|------|------|
| `400` | è¯·æ±‚å‚æ•°é”™è¯¯ | æ£€æŸ¥è¯·æ±‚ä½“æ ¼å¼ |
| `404` | æ¨¡å‹ä¸å­˜åœ¨ | ç¡®è®¤æ¨¡å‹åç§°æ­£ç¡® |
| `500` | æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ | æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€ |
| `503` | OllamaæœåŠ¡ä¸å¯ç”¨ | å¯åŠ¨OllamaæœåŠ¡ |
| `422` | è¯·æ±‚ä½“éªŒè¯å¤±è´¥ | æ£€æŸ¥å¿…éœ€å‚æ•° |

## ğŸ› ï¸ ä½¿ç”¨ç¤ºä¾‹

### JavaScript/Fetchç¤ºä¾‹

```javascript
// è·å–æ¨¡å‹åˆ—è¡¨
async function getModels() {
  const response = await fetch('http://localhost:8000/v1/models');
  const data = await response.json();
  return data.data;
}

// å‘é€èŠå¤©æ¶ˆæ¯
async function sendChat(message) {
  const response = await fetch('http://localhost:8000/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'llama2',
      messages: [
        { role: 'user', content: message }
      ],
      temperature: 0.7,
      max_tokens: 1000
    })
  });
  
  const data = await response.json();
  return data.choices[0].message.content;
}

// æµå¼èŠå¤©
async function streamChat(message, onChunk) {
  const response = await fetch('http://localhost:8000/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Accept': 'text/event-stream'
    },
    body: JSON.stringify({
      model: 'llama2',
      messages: [{ role: 'user', content: message }],
      stream: true
    })
  });

  const reader = response.body.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value);
    const lines = chunk.split('\n');
    
    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.slice(6);
        if (data === '[DONE]') return;
        
        try {
          const parsed = JSON.parse(data);
          const content = parsed.choices[0]?.delta?.content;
          if (content) onChunk(content);
        } catch (e) {
          // å¿½ç•¥è§£æé”™è¯¯
        }
      }
    }
  }
}
```

### Pythonç¤ºä¾‹

```python
import requests
import json

# è·å–æ¨¡å‹åˆ—è¡¨
def get_models():
    response = requests.get('http://localhost:8000/v1/models')
    return response.json()['data']

# å‘é€èŠå¤©æ¶ˆæ¯
def send_chat(message):
    payload = {
        'model': 'llama2',
        'messages': [
            {'role': 'user', 'content': message}
        ],
        'temperature': 0.7,
        'max_tokens': 1000
    }
    
    response = requests.post(
        'http://localhost:8000/v1/chat/completions',
        headers={'Content-Type': 'application/json'},
        data=json.dumps(payload)
    )
    
    data = response.json()
    return data['choices'][0]['message']['content']

# æµå¼èŠå¤©
def stream_chat(message):
    payload = {
        'model': 'llama2',
        'messages': [{'role': 'user', 'content': message}],
        'stream': True
    }
    
    response = requests.post(
        'http://localhost:8000/v1/chat/completions',
        headers={
            'Content-Type': 'application/json',
            'Accept': 'text/event-stream'
        },
        data=json.dumps(payload),
        stream=True
    )
    
    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            if line.startswith('data: '):
                data = line[6:]
                if data == '[DONE]':
                    break
                try:
                    parsed = json.loads(data)
                    content = parsed['choices'][0]['delta'].get('content')
                    if content:
                        print(content, end='', flush=True)
                except json.JSONDecodeError:
                    continue
```

### cURLç¤ºä¾‹

```bash
# è·å–æ¨¡å‹åˆ—è¡¨
curl http://localhost:8000/v1/models

# å‘é€èŠå¤©æ¶ˆæ¯
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 1000
  }'

# æµå¼èŠå¤©
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -d '{
    "model": "llama2",
    "messages": [
      {"role": "user", "content": "Tell me a story"}
    ],
    "stream": true
  }'

# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health
```

---

## ğŸ“ æ”¯æŒ

å¦‚æœ‰APIä½¿ç”¨é—®é¢˜ï¼Œè¯·å‚è€ƒï¼š
- [ä¸»è¦æ–‡æ¡£](./README.md) - åŸºç¡€ä½¿ç”¨æŒ‡å—
- [å¼€å‘è€…æŒ‡å—](./DEVELOPER_GUIDE.md) - è¯¦ç»†å¼€å‘è¯´æ˜
- [GitHub Issues](https://github.com/yourusername/praise-ai/issues) - é—®é¢˜æŠ¥å‘Š

**APIç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025å¹´8æœˆ